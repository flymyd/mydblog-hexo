---
title: 生成式人工智能入门
toc: true
date: 2024-09-26 15:41:01
tags:
categories:
---
# 前言

以下是文章中可能提及到的关键词及对应缩写：

| 中文名                 | 英文名                                     | 缩写 |
| :--------------------- | :----------------------------------------- | :--- |
| 人工智能               | Artificial Intelligence                    | AI   |
| 生成式人工智能         | Generative Artificial Intelligence         | GAI  |
| 通用人工智能           | Artificial General Intelligence            | AGI  |
| 人工智能生成内容       | AI Generated Content                       | AIGC |
| 基于人类反馈的强化学习 | Reinforcement Learning with Human Feedback | RLHF |
| 大语言模型             | Large Language Model                       | LLM  |

# 一、概述  

2022年11月30日，ChatGPT上线。马斯克在12月3日发布了一条twitter，成为了ChatGPT火遍全球的导火索，火到自11月30日上线不到5天就突破百万人的注册使用。  

## ChatGPT、GPT和LLM分别是什么？它们的关系是什么？
- GPT即Generative Pre-trained Transformer。Generative 即生成式，指该模型可以生成新的内容；Pre-trained 即预训练，指该模型可以进行预先训练；Transformer 则是近年来主流的深度学习模型。GPT取得突破的一大原因，是引入了新技术RLHF（基于人类反馈的强化学习），使人工智能模型的输出内容和人类的常识、认知、需求、价值观等相一致， 让人难以分辨输出者是人类还是机器。
- ChatGPT则是由OpenAI公司开发的一个人工智能聊天机器人程序。它能够通过学习和理解人类的语言来进行对话互动交流，甚至能完成撰写邮件、文案、翻译、代码等任务。ChatGPT背后的驱动模型分为很多型号。对于对话交流能力来说，驱动它的模型属于GPT模型。
- LLM，即大语言模型，旨在理解和生成人类语言。LLM的特点是规模庞大，包含成百、上千亿的参数，可以捕捉语言的复杂模式，包括句法、语义和一些上下文信息，从而生成连贯的、有意义的文本。ChatGPT、GPT-4、BERT、文心一言等都是典型的大型语言模型。
  所以说，并不是所有的LLM服务都叫做ChatGPT。
  
## GAI、AGI和AIGC分别是什么？它们的关系是什么？  
- GAI，即生成式人工智能，这是一种利用AI生成新的、未曾存在的数据（如文本、图像、音乐等）的技术。GAI强调的是通过机器学习模型，特别是深度学习模型如生成对抗网络（GANs）或变分自编码器（VAEs），来创造出贴近或超越现实质量的内容。GAI的应用示例包括生成逼真的图像、创作音乐、写作故事等。例如：Stable Diffusion、Midjourney等AI绘图应用和ChatGPT、Claude等AI对话应用等。
- AGI，即通用人工智能，指的是一种理论上的AI系统，这种系统能够理解、学习和应用任何智力任务，就像一个成人或超越人类智能水平一样。AGI具有广泛的适应性和泛化能力，能够在没有特定培训的情况下执行各种复杂任务。AGI的目标是创造一个能像人类一样思考、学习、执行多种任务的系统，成为全能的“超级大脑”，未来可能在任何领域都超越人类。
- AIGC，即AI生成内容，聚焦于使用人工智能生成内容的领域，可以看作是GAI的一个子集或应用领域。它主要关注利用AI技术自动生成具有创造性的内容，如文本、图像、音乐等。AIGC的目标是提高内容创作的质量和效率，通常依赖于特定的数据和算法模型来实现。
- GAI与AIGC的关系较为紧密，因为GAI的生成式技术是实现AIGC的关键。GAI的技术和算法为AIGC提供了生成新颖内容的能力。
- AGI与GAI和AIGC的关系较为宽泛，因为AGI的目标是创建一种具有全面理解和学习能力的智能系统，而不仅仅是生成特定类型的内容。AGI在理论上能够执行GAI和AIGC的任务，但它的目标和应用范围更广泛。

## 大模型与小模型
关于大模型，有学者称之为“大规模预训练模型”(large pretrained language model），也有学者进一步提出”基础模型”(Foundation Models)的概念。
2021年8月，李飞飞、Percy Liang等百来位学者联名发布了文章：On the Opportunities and Risks of Foundation Models[1]，提出“基础模型”(Foundation Models)的概念：基于自监督学习的模型在学习过程中会体现出来各个不同方面的能力，这些能力为下游的应用提供了动力和理论基础，称这些大模型为“基础模型”。
- 小模型：针对特定应用场景需求进行训练，能完成特定任务，但是换到另外一个应用场景中可能并不适用，需要重新训练（我们现在用的大多数模型都是这样）。这些模型训练基本是“手工作坊式”，并且模型训练需要大规模的标注数据，如果某些应用场景的数据量少，训练出的模型精度就会不理想。
- 大模型：在大规模无标注数据上进行训练，学习出一种特征和规则。基于大模型进行应用开发时，将大模型进行微调（在下游小规模有标注数据进行二次训练）或者不进行微调，就可以完成多个应用场景的任务，实现通用的智能能力。可以这么类别，机器学习同质化学习算法（例如逻辑回归）、深度学习同质化模型结构（例如CNN），基础模型则同质化模型本身（例如GPT-3）。

## AIGC都能做什么？
下面示例了AIGC能做的事情：

![f77999223bce7941f05c45a0dec029d9](f77999223bce7941f05c45a0dec029d9.png)

# 二、大语言模型（LLM）

## 技术介绍

LLM，即大型语言模型（Large Language Models），是人工智能领域的一种先进自然语言处理技术，它们通过深度学习方法，特别是基于Transformer架构的神经网络模型，来理解和生成人类语言。这类模型因其强大的语言理解和生成能力，在诸多应用场景中展现出巨大潜力，如自动回复、文本创作、机器翻译、摘要生成等。

### 技术基础

1. 深度学习与神经网络：LLM的核心是深度学习，尤其是使用了Transformer架构的神经网络。Transformer模型最初由Vaswani等人在2017年提出，它摒弃了传统的循环网络结构，通过自注意力（Self-Attention）机制并行处理输入序列，大大提高了处理速度和模型容量。
2. 大规模预训练：LLM的一个关键特点是其基于大规模无监督数据的预训练。模型在互联网文本、书籍、新闻等海量数据上进行训练，学习语言的一般规律和模式，而不是针对特定任务。这使得模型能够捕捉到丰富的语言结构和语义信息。
3. 微调（Fine-tuning）：虽然预训练赋予了模型广泛的语言理解能力，但针对特定任务时，通常会对预训练好的模型进行微调，即在特定领域的有标签数据集上进行额外训练，以优化模型在该任务上的性能。

### 原理概述

1. 自注意力机制：这是Transformer模型的核心组件，允许模型在处理一个词时，考虑句子中所有其他词的上下文，从而更好地理解词汇之间的关系和依赖。
2. 多层编码解码：LLM通常包含多层的编码器和解码器结构。编码器负责将输入文本转换为一系列连续的向量表示，而解码器则基于这些向量生成输出文本。每一层都包含自注意力和前馈神经网络组件，层层深入地提取和处理语言特征。
3. 位置编码：由于Transformer模型本身不保留输入序列的顺序信息，因此引入了位置编码机制，确保模型能够区分不同位置的词语，正确理解语序。
4. 海量参数：现代的LLM，如GPT-3、BERT等，拥有数十亿乃至上千亿的参数量。这些庞大的参数使模型能够学习到更复杂、更细腻的语言规律，实现更高质量的语言生成和理解。

## 一些AI资源站

- AI工具集（https://ai-bot.cn/）
- AI工具箱（https://www.phpcms9.com/ai/chatgpt）
- AI工具箱2（https://www.ailookme.com/）
